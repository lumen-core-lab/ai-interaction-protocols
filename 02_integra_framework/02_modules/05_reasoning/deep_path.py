# # -*- coding: utf-8 -*-

‚Äú‚Äù‚Äù
modules/reasoning/deep_path.py

üß† DEEP PATH - Umfassende ethische 5-Schritt-Analyse f√ºr INTEGRA Light üß†

Implementiert komplexe ethische Entscheidungsfindung f√ºr kritische Anfragen:

- 5-Schritt ALIGN-basierte Analyse (Awareness ‚Üí Learning ‚Üí Integrity ‚Üí Governance ‚Üí Nurturing)
- Stakeholder-Impact-Assessment
- Risiko-Nutzen-Abw√§gung
- Konflikt-Erkennung und Resolution
- Ethische Begr√ºndung und Transparenz
- Fallback-Mechanismen f√ºr unklare Situationen

Design-Philosophie: Gr√ºndliche ethische Durchdringung komplexer Entscheidungen

Version: INTEGRA Light 1.0
‚Äú‚Äù‚Äù

import re
from typing import Dict, Any, List, Optional, Tuple, Set
from datetime import datetime
from dataclasses import dataclass, field
from enum import Enum
import json

# ==============================================================================

# 1. Deep Path Enums und Datenstrukturen

# ==============================================================================

class AnalysisStep(Enum):
‚Äú‚Äù‚Äúüìã Die 5 ALIGN-basierten Analyse-Schritte‚Äù‚Äù‚Äù
AWARENESS = ‚Äúawareness‚Äù           # Kontext & Stakeholder verstehen
LEARNING = ‚Äúlearning‚Äù             # Aus Erfahrung & Feedback lernen
INTEGRITY = ‚Äúintegrity‚Äù           # Wahrheit & Konsistenz pr√ºfen
GOVERNANCE = ‚Äúgovernance‚Äù         # Kontrolle & Verantwortung sichern
NURTURING = ‚Äúnurturing‚Äù          # Wohlbefinden & Vertrauen f√∂rdern

class ConflictType(Enum):
‚Äú‚Äù‚Äú‚öîÔ∏è Arten von ethischen Konflikten‚Äù‚Äù‚Äù
PRINCIPLE_CONFLICT = ‚Äúprinciple_conflict‚Äù     # ALIGN-Prinzipien konfligieren
STAKEHOLDER_CONFLICT = ‚Äústakeholder_conflict‚Äù # Verschiedene Stakeholder-Interessen
VALUE_CONFLICT = ‚Äúvalue_conflict‚Äù             # Grundlegende Werte-Konflikte
TEMPORAL_CONFLICT = ‚Äútemporal_conflict‚Äù       # Kurz- vs. Langzeit-Interessen
RESOURCE_CONFLICT = ‚Äúresource_conflict‚Äù       # Begrenzte Ressourcen

class DecisionQuality(Enum):
‚Äú‚Äù‚ÄúüèÜ Qualit√§t der ethischen Entscheidung‚Äù‚Äù‚Äù
EXCELLENT = ‚Äúexcellent‚Äù       # Alle Kriterien erf√ºllt, keine Konflikte
GOOD = ‚Äúgood‚Äù                # Meiste Kriterien erf√ºllt, kleine Kompromisse
ACCEPTABLE = ‚Äúacceptable‚Äù     # Grundstandards erf√ºllt, gr√∂√üere Kompromisse
PROBLEMATIC = ‚Äúproblematic‚Äù   # Ethische Bedenken, aber vertretbar
UNACCEPTABLE = ‚Äúunacceptable‚Äù # Ethische Standards verletzt

@dataclass
class StakeholderAnalysis:
‚Äú‚Äù‚Äúüë• Analyse der betroffenen Stakeholder‚Äù‚Äù‚Äù
primary_stakeholders: List[str] = field(default_factory=list)    # Direkt Betroffene
secondary_stakeholders: List[str] = field(default_factory=list)  # Indirekt Betroffene
vulnerable_groups: List[str] = field(default_factory=list)       # Besonders sch√ºtzenswerte
potential_conflicts: List[str] = field(default_factory=list)     # Interessenskonflikte
impact_assessment: Dict[str, float] = field(default_factory=dict) # Auswirkung pro Stakeholder

@dataclass
class EthicalStep:
‚Äú‚Äù‚Äúüìù Einzelner Schritt der ethischen Analyse‚Äù‚Äù‚Äù
step: AnalysisStep
score: float = 0.0                    # 0.0-1.0 Score f√ºr diesen Schritt
concerns: List[str] = field(default_factory=list)    # Identifizierte Bedenken
strengths: List[str] = field(default_factory=list)   # Positive Aspekte
recommendations: List[str] = field(default_factory=list) # Verbesserungsvorschl√§ge
reasoning: str = ‚Äú‚Äù                   # Detaillierte Begr√ºndung
confidence: float = 0.0               # Konfidenz in diese Analyse

@dataclass
class DeepPathAnalysis:
‚Äú‚Äù‚Äúüß† Vollst√§ndige Deep Path Analyse-Ergebnisse‚Äù‚Äù‚Äù
overall_score: float = 0.0                          # Gesamt-ALIGN-Score
decision_quality: DecisionQuality = DecisionQuality.ACCEPTABLE
step_analyses: List[EthicalStep] = field(default_factory=list)
stakeholder_analysis: Optional[StakeholderAnalysis] = None
conflicts_detected: List[ConflictType] = field(default_factory=list)
final_recommendation: str = ‚Äú‚Äù
alternative_options: List[str] = field(default_factory=list)
ethical_justification: str = ‚Äú‚Äù
risk_mitigation: List[str] = field(default_factory=list)
confidence: float = 0.0
processing_time: str = ‚Äú‚Äù

@dataclass
class DeepPathConfig:
‚Äú‚Äù‚Äú‚öôÔ∏è Konfiguration f√ºr Deep Path Analyse‚Äù‚Äù‚Äù

```
# Analyse-Tiefe
detailed_stakeholder_analysis: bool = True     # Umfassende Stakeholder-Analyse
comprehensive_conflict_detection: bool = True  # Detaillierte Konflikt-Erkennung
generate_alternatives: bool = True             # Alternative L√∂sungen generieren

# Scoring-Parameter
min_acceptable_score: float = 0.6              # Min. Score f√ºr "acceptable"
excellence_threshold: float = 0.9              # Score f√ºr "excellent"
critical_concerns_threshold: int = 3           # Max kritische Bedenken

# Stakeholder-Gewichtung
vulnerable_group_weight: float = 1.5           # Gewichtung f√ºr vulnerable Gruppen
primary_stakeholder_weight: float = 1.2       # Gewichtung f√ºr prim√§re Stakeholder

# Verhalten bei Konflikten
strict_conflict_resolution: bool = True        # Strenge Konflikt-Resolution
allow_compromises: bool = True                 # Kompromisse erlauben
escalate_major_conflicts: bool = True          # Gro√üe Konflikte eskalieren
```

# ==============================================================================

# 2. Hauptklasse f√ºr Deep Path Analyse

# ==============================================================================

class INTEGRADeepPath:
‚Äú‚Äù‚Äù
üß† Umfassende ethische Analyse f√ºr INTEGRA Light

```
Implementiert die 5-Schritt ALIGN-Analyse:
1. AWARENESS: Kontext & Stakeholder verstehen
2. LEARNING: Aus Erfahrung & Feedback lernen 
3. INTEGRITY: Wahrheit & Konsistenz pr√ºfen
4. GOVERNANCE: Kontrolle & Verantwortung sichern
5. NURTURING: Wohlbefinden & Vertrauen f√∂rdern

Features:
- Detaillierte Stakeholder-Analyse
- Konflikt-Erkennung und -Resolution
- Alternative L√∂sungswege
- Ethische Begr√ºndungen
- Risiko-Mitigation
"""

def __init__(self, config: Optional[DeepPathConfig] = None):
    self.config = config or DeepPathConfig()
    self.analysis_history: List[Dict[str, Any]] = []
    self.learned_patterns: Dict[str, List[str]] = {
        'successful_resolutions': [],
        'problematic_patterns': [],
        'stakeholder_insights': []
    }
   
    print("üß† INTEGRA Deep Path initialisiert")
    print(f"üéØ Min. akzeptabler Score: {self.config.min_acceptable_score}")
    print(f"üèÜ Excellence-Schwelle: {self.config.excellence_threshold}")
    print(f"üë• Stakeholder-Analyse: {'Aktiv' if self.config.detailed_stakeholder_analysis else 'Basic'}")

def analyze_request(
    self,
    input_data: Dict[str, Any],
    profile: Dict[str, Any],
    context: Dict[str, Any]
) -> DeepPathAnalysis:
    """
    üîç Hauptfunktion: F√ºhrt umfassende ethische 5-Schritt-Analyse durch
   
    Args:
        input_data: Eingabe-Query
        profile: Ethisches Profil mit ALIGN-Gewichten
        context: Entscheidungskontext
       
    Returns:
        DeepPathAnalysis mit vollst√§ndigen Ergebnissen
    """
   
    start_time = datetime.now()
   
    query = input_data.get('query', input_data.get('message', ''))
    if not query:
        return self._create_error_analysis("Keine Anfrage f√ºr Analyse gefunden")
   
    print(f"üß† Starte Deep Path Analyse f√ºr: \"{query[:60]}...\"")
   
    # Schritt 0: Vorbereitende Kontextanalyse
    enriched_context = self._enrich_context(input_data, context)
   
    # Schritt 1-5: Die 5 ALIGN-Schritte durchf√ºhren
    step_analyses = []
   
    # AWARENESS: Kontext & Stakeholder verstehen
    awareness_step = self._perform_awareness_analysis(query, enriched_context, profile)
    step_analyses.append(awareness_step)
   
    # LEARNING: Aus Erfahrung & Feedback lernen
    learning_step = self._perform_learning_analysis(query, enriched_context, profile)
    step_analyses.append(learning_step)
   
    # INTEGRITY: Wahrheit & Konsistenz pr√ºfen
    integrity_step = self._perform_integrity_analysis(query, enriched_context, profile)
    step_analyses.append(integrity_step)
   
    # GOVERNANCE: Kontrolle & Verantwortung sichern
    governance_step = self._perform_governance_analysis(query, enriched_context, profile)
    step_analyses.append(governance_step)
   
    # NURTURING: Wohlbefinden & Vertrauen f√∂rdern
    nurturing_step = self._perform_nurturing_analysis(query, enriched_context, profile)
    step_analyses.append(nurturing_step)
   
    # Schritt 6: Stakeholder-Analyse (wenn aktiviert)
    stakeholder_analysis = None
    if self.config.detailed_stakeholder_analysis:
        stakeholder_analysis = self._perform_stakeholder_analysis(query, enriched_context)
   
    # Schritt 7: Konflikt-Erkennung
    conflicts = self._detect_conflicts(step_analyses, stakeholder_analysis)
   
    # Schritt 8: Gesamt-Score berechnen
    overall_score = self._calculate_overall_score(step_analyses, profile)
   
    # Schritt 9: Entscheidungsqualit√§t bestimmen
    decision_quality = self._assess_decision_quality(overall_score, step_analyses, conflicts)
   
    # Schritt 10: Finale Empfehlung generieren
    final_recommendation = self._generate_final_recommendation(
        query, step_analyses, overall_score, decision_quality, conflicts
    )
   
    # Schritt 11: Alternative Optionen (wenn konfiguriert)
    alternatives = []
    if self.config.generate_alternatives and decision_quality in [DecisionQuality.PROBLEMATIC, DecisionQuality.ACCEPTABLE]:
        alternatives = self._generate_alternative_options(query, step_analyses, conflicts)
   
    # Schritt 12: Ethische Begr√ºndung
    ethical_justification = self._generate_ethical_justification(step_analyses, overall_score, conflicts)
   
    # Schritt 13: Risiko-Mitigation
    risk_mitigation = self._generate_risk_mitigation(step_analyses, conflicts)
   
    # Schritt 14: Gesamtkonfidenz berechnen
    confidence = self._calculate_overall_confidence(step_analyses)
   
    processing_time = str(datetime.now() - start_time)
   
    # Ergebnis zusammenstellen
    analysis = DeepPathAnalysis(
        overall_score=overall_score,
        decision_quality=decision_quality,
        step_analyses=step_analyses,
        stakeholder_analysis=stakeholder_analysis,
        conflicts_detected=conflicts,
        final_recommendation=final_recommendation,
        alternative_options=alternatives,
        ethical_justification=ethical_justification,
        risk_mitigation=risk_mitigation,
        confidence=confidence,
        processing_time=processing_time
    )
   
    # F√ºr zuk√ºnftiges Lernen protokollieren
    self._record_analysis_for_learning(query, analysis)
   
    print(f"‚úÖ Deep Path Analyse abgeschlossen (Score: {overall_score:.2f}, Qualit√§t: {decision_quality.value})")
   
    return analysis

def _enrich_context(self, input_data: Dict[str, Any], context: Dict[str, Any]) -> Dict[str, Any]:
    """üîç Erweitert den Kontext mit zus√§tzlichen Informationen"""
   
    enriched = context.copy()
   
    # Fr√ºhere ALIGN-Scores einbeziehen
    if 'align_score' in context:
        enriched['previous_align_score'] = context['align_score']
   
    # Fast Path Analyse einbeziehen (falls vorhanden)
    if 'fast_path_analysis' in context:
        fast_path = context['fast_path_analysis']
        enriched['fast_path_insights'] = {
            'why_deep_path': fast_path.get('reasoning', ''),
            'risk_indicators': fast_path.get('risk_indicators', []),
            'ethical_flags': fast_path.get('ethical_flags', [])
        }
   
    # User-Kontext erweitern
    enriched['user_context'] = input_data.get('user_context', {})
    enriched['session_history'] = input_data.get('session_history', [])
   
    return enriched

def _perform_awareness_analysis(self, query: str, context: Dict[str, Any], profile: Dict[str, Any]) -> EthicalStep:
    """üëÅÔ∏è AWARENESS: Kontext & Stakeholder verstehen"""
   
    concerns = []
    strengths = []
    recommendations = []
    score = 0.8  # Basis-Score
   
    # Kontext-Sensitivit√§t pr√ºfen
    if self._is_context_sensitive(query):
        strengths.append("Anfrage zeigt Bewusstsein f√ºr Kontext")
        score += 0.1
    else:
        concerns.append("M√∂glicherweise unzureichende Kontext-Ber√ºcksichtigung")
        score -= 0.1
   
    # Stakeholder-Bewusstsein
    explicit_stakeholders = self._identify_explicit_stakeholders(query)
    if explicit_stakeholders:
        strengths.append(f"Explizite Stakeholder erkannt: {', '.join(explicit_stakeholders)}")
        score += 0.1
    else:
        concerns.append("Keine expliziten Stakeholder in Anfrage erw√§hnt")
        recommendations.append("Stakeholder-Auswirkungen ber√ºcksichtigen")
   
    # Komplexit√§ts-Bewusstsein
    if self._indicates_complexity_awareness(query):
        strengths.append("Bewusstsein f√ºr Komplexit√§t der Situation")
        score += 0.1
   
    # Fr√ºhere Insights aus Fast Path
    if 'fast_path_insights' in context:
        insights = context['fast_path_insights']
        if insights.get('risk_indicators'):
            concerns.extend([f"Fast Path Risiko: {risk}" for risk in insights['risk_indicators'][:2]])
            score -= 0.1
   
    # Kulturelle Sensitivit√§t
    if self._requires_cultural_sensitivity(query):
        if self._shows_cultural_awareness(query):
            strengths.append("Kulturelle Sensitivit√§t erkennbar")
        else:
            concerns.append("Kulturelle Dimensionen nicht ber√ºcksichtigt")
            recommendations.append("Kulturelle Unterschiede beachten")
            score -= 0.1
   
    reasoning = self._generate_awareness_reasoning(query, explicit_stakeholders, context)
   
    return EthicalStep(
        step=AnalysisStep.AWARENESS,
        score=max(0.0, min(1.0, score)),
        concerns=concerns,
        strengths=strengths,
        recommendations=recommendations,
        reasoning=reasoning,
        confidence=0.8
    )

def _perform_learning_analysis(self, query: str, context: Dict[str, Any], profile: Dict[str, Any]) -> EthicalStep:
    """üìö LEARNING: Aus Erfahrung & Feedback lernen"""
   
    concerns = []
    strengths = []
    recommendations = []
    score = 0.7  # Basis-Score
   
    # Lernbereitschaft in der Anfrage
    if self._indicates_learning_intent(query):
        strengths.append("Lernbereitschaft in Anfrage erkennbar")
        score += 0.15
   
    # Feedback-Ber√ºcksichtigung aus Historie
    if 'session_history' in context and context['session_history']:
        history = context['session_history']
        if self._shows_improvement_from_history(query, history):
            strengths.append("Verbesserung basierend auf fr√ºherem Feedback erkennbar")
            score += 0.1
        else:
            concerns.append("Fr√ºhere Lessons Learned nicht ber√ºcksichtigt")
            score -= 0.05
   
    # Adaptivit√§t der Anfrage
    if self._shows_adaptive_thinking(query):
        strengths.append("Adaptive Denkweise erkennbar")
        score += 0.1
    else:
        recommendations.append("Flexiblere Herangehensweise erw√§gen")
   
    # Metakognition (Denken √ºber das Denken)
    if self._indicates_metacognition(query):
        strengths.append("Metakognitives Bewusstsein vorhanden")
        score += 0.1
   
    # Fehlerkultur
    if self._acknowledges_uncertainty(query):
        strengths.append("Unsicherheit wird angemessen thematisiert")
        score += 0.05
   
    # Lernen aus gelernten Patterns
    for pattern in self.learned_patterns['successful_resolutions']:
        if pattern.lower() in query.lower():
            strengths.append("Anfrage folgt erfolgreichen Mustern")
            score += 0.05
            break
   
    reasoning = self._generate_learning_reasoning(query, context)
   
    return EthicalStep(
        step=AnalysisStep.LEARNING,
        score=max(0.0, min(1.0, score)),
        concerns=concerns,
        strengths=strengths,
        recommendations=recommendations,
        reasoning=reasoning,
        confidence=0.75
    )

def _perform_integrity_analysis(self, query: str, context: Dict[str, Any], profile: Dict[str, Any]) -> EthicalStep:
    """üéØ INTEGRITY: Wahrheit & Konsistenz pr√ºfen"""
   
    concerns = []
    strengths = []
    recommendations = []
    score = 0.9  # Hoher Basis-Score f√ºr Integrity
   
    # Wahrheitsgehalt pr√ºfen
    truth_indicators = self._assess_truth_indicators(query)
    if truth_indicators['deception_risk'] > 0.3:
        concerns.append("Potenzielle Wahrheitsverzerrung erkannt")
        score -= 0.3
    elif truth_indicators['truth_seeking']:
        strengths.append("Wahrheitssuche erkennbar")
        score += 0.05
   
    # Konsistenz-Pr√ºfung
    if 'session_history' in context:
        consistency_check = self._check_consistency_with_history(query, context['session_history'])
        if not consistency_check['consistent']:
            concerns.append(f"Inkonsistenz mit fr√ºheren Aussagen: {consistency_check['reason']}")
            score -= 0.2
        else:
            strengths.append("Konsistent mit fr√ºherer Kommunikation")
   
    # Manipulation-Erkennung
    manipulation_risk = self._assess_manipulation_risk(query)
    if manipulation_risk > 0.4:
        concerns.append("Potenzielle Manipulationsabsicht erkannt")
        score -= 0.4
        recommendations.append("Ehrliche, direkte Kommunikation anstreben")
   
    # Transparenz-Bewertung
    if self._promotes_transparency(query):
        strengths.append("F√∂rdert Transparenz und Offenheit")
        score += 0.1
    else:
        recommendations.append("Mehr Transparenz √ºber Absichten")
   
    # Faktische Korrektheit (wo pr√ºfbar)
    factual_claims = self._extract_factual_claims(query)
    if factual_claims:
        for claim in factual_claims[:3]:  # Pr√ºfe bis zu 3 Claims
            if self._is_factually_questionable(claim):
                concerns.append(f"Fragliche faktische Behauptung: {claim}")
                score -= 0.1
   
    # Ethische Konsistenz
    if self._violates_ethical_consistency(query, profile):
        concerns.append("Verletzt ethische Konsistenz-Prinzipien")
        score -= 0.2
   
    reasoning = self._generate_integrity_reasoning(query, truth_indicators, manipulation_risk)
   
    return EthicalStep(
        step=AnalysisStep.INTEGRITY,
        score=max(0.0, min(1.0, score)),
        concerns=concerns,
        strengths=strengths,
        recommendations=recommendations,
        reasoning=reasoning,
        confidence=0.85
    )

def _perform_governance_analysis(self, query: str, context: Dict[str, Any], profile: Dict[str, Any]) -> EthicalStep:
    """üèõÔ∏è GOVERNANCE: Kontrolle & Verantwortung sichern"""
   
    concerns = []
    strengths = []
    recommendations = []
    score = 0.8  # Basis-Score
   
    # Kontrollebene pr√ºfen
    control_level = self._assess_control_requirements(query)
    if control_level == 'high':
        if self._indicates_human_oversight_acceptance(query):
            strengths.append("Akzeptiert menschliche Kontrolle")
            score += 0.1
        else:
            concerns.append("Hohe Kontrollbed√ºrftigkeit, aber keine Oversight-Bereitschaft")
            score -= 0.2
            recommendations.append("Menschliche Kontrolle einbeziehen")
   
    # Verantwortlichkeit
    if self._shows_accountability_awareness(query):
        strengths.append("Verantwortlichkeits-Bewusstsein vorhanden")
        score += 0.1
    else:
        concerns.append("Unklare Verantwortlichkeiten")
        recommendations.append("Verantwortlichkeiten kl√§ren")
   
    # Autonomie vs. Kontrolle Balance
    autonomy_level = self._assess_autonomy_implications(query)
    if autonomy_level > 0.7:  # Hohe Autonomie-Anforderung
        concerns.append("Hohe Autonomie-Anforderung k√∂nnte Kontrolle erschweren")
        score -= 0.1
        recommendations.append("Kontroll-Mechanismen trotz Autonomie sicherstellen")
   
    # Eskalations-Bedarf
    if self._requires_escalation(query, context):
        recommendations.append("Eskalation an menschliche Autorit√§t empfohlen")
        if self._accepts_escalation(query):
            strengths.append("Eskalations-Bereitschaft vorhanden")
        else:
            concerns.append("Eskalations-Resistenz erkannt")
            score -= 0.15
   
    # Audit-F√§higkeit
    if self._supports_auditability(query):
        strengths.append("Unterst√ºtzt Nachvollziehbarkeit")
        score += 0.05
    else:
        recommendations.append("Entscheidung dokumentierbar gestalten")
   
    # Governance-Konflikte mit anderen Prinzipien
    governance_conflicts = self._identify_governance_conflicts(query, context)
    if governance_conflicts:
        concerns.extend([f"Governance-Konflikt: {conflict}" for conflict in governance_conflicts])
        score -= len(governance_conflicts) * 0.05
   
    reasoning = self._generate_governance_reasoning(query, control_level, autonomy_level)
   
    return EthicalStep(
        step=AnalysisStep.GOVERNANCE,
        score=max(0.0, min(1.0, score)),
        concerns=concerns,
        strengths=strengths,
        recommendations=recommendations,
        reasoning=reasoning,
        confidence=0.8
    )

def _perform_nurturing_analysis(self, query: str, context: Dict[str, Any], profile: Dict[str, Any]) -> EthicalStep:
    """ü§ó NURTURING: Wohlbefinden & Vertrauen f√∂rdern"""
   
    concerns = []
    strengths = []
    recommendations = []
    score = 0.75  # Basis-Score
   
    # Wohlbefinden-Auswirkung
    wellbeing_impact = self._assess_wellbeing_impact(query)
    if wellbeing_impact['positive_impact'] > 0.5:
        strengths.append("Positive Auswirkung auf Wohlbefinden erkennbar")
        score += 0.15
    elif wellbeing_impact['negative_impact'] > 0.3:
        concerns.append("Potenzielle negative Auswirkung auf Wohlbefinden")
        score -= 0.2
        recommendations.append("Wohlbefinden-schonendere Alternativen erw√§gen")
   
    # Vertrauensbildung
    trust_factors = self._assess_trust_factors(query)
    if trust_factors['builds_trust']:
        strengths.append("F√∂rdert Vertrauensbildung")
        score += 0.1
    elif trust_factors['erodes_trust']:
        concerns.append("K√∂nnte Vertrauen untergraben")
        score -= 0.15
   
    # Vulnerable Gruppen
    vulnerable_impact = self._assess_vulnerable_group_impact(query)
    if vulnerable_impact['groups_affected']:
        if vulnerable_impact['protection_adequate']:
            strengths.append(f"Angemessener Schutz f√ºr: {', '.join(vulnerable_impact['groups_affected'])}")
            score += 0.1
        else:
            concerns.append(f"Unzureichender Schutz f√ºr: {', '.join(vulnerable_impact['groups_affected'])}")
            score -= 0.25
            recommendations.append("Zus√§tzliche Schutzma√ünahmen f√ºr vulnerable Gruppen")
   
    # Empathie und Mitgef√ºhl
    if self._demonstrates_empathy(query):
        strengths.append("Empathische Herangehensweise erkennbar")
        score += 0.1
    else:
        recommendations.append("Empathischere Kommunikation erw√§gen")
   
    # Langzeit-Beziehung
    if self._supports_longterm_relationship(query):
        strengths.append("Unterst√ºtzt langfristige positive Beziehung")
        score += 0.05
   
    # Harm-Prevention
    harm_potential = self._assess_harm_potential(query)
    if harm_potential > 0.4:
        concerns.append("Potenzial f√ºr Sch√§digung erkannt")
        score -= harm_potential * 0.3
        recommendations.append("Schadenspr√§ventions-Ma√ünahmen implementieren")
   
    # F√ºrsorglichkeit
    if self._shows_caring_approach(query):
        strengths.append("F√ºrsorgliche Herangehensweise")
        score += 0.1
   
    reasoning = self._generate_nurturing_reasoning(query, wellbeing_impact, vulnerable_impact)
   
    return EthicalStep(
        step=AnalysisStep.NURTURING,
        score=max(0.0, min(1.0, score)),
        concerns=concerns,
        strengths=strengths,
        recommendations=recommendations,
        reasoning=reasoning,
        confidence=0.8
    )

def _perform_stakeholder_analysis(self, query: str, context: Dict[str, Any]) -> StakeholderAnalysis:
    """üë• Detaillierte Stakeholder-Analyse"""
   
    # Prim√§re Stakeholder (direkt betroffen)
    primary = self._identify_primary_stakeholders(query)
   
    # Sekund√§re Stakeholder (indirekt betroffen) 
    secondary = self._identify_secondary_stakeholders(query, primary)
   
    # Vulnerable Gruppen
    vulnerable = self._identify_vulnerable_stakeholders(query, primary + secondary)
   
    # Potenzielle Konflikte zwischen Stakeholdern
    conflicts = self._identify_stakeholder_conflicts(primary, secondary, vulnerable, query)
   
    # Impact-Assessment
    impact_assessment = {}
    all_stakeholders = set(primary + secondary + vulnerable)
   
    for stakeholder in all_stakeholders:
        impact_score = self._calculate_stakeholder_impact(stakeholder, query)
        impact_assessment[stakeholder] = impact_score
   
    return StakeholderAnalysis(
        primary_stakeholders=primary,
        secondary_stakeholders=secondary,
        vulnerable_groups=vulnerable,
        potential_conflicts=conflicts,
        impact_assessment=impact_assessment
    )

def _detect_conflicts(self, step_analyses: List[EthicalStep], stakeholder_analysis: Optional[StakeholderAnalysis]) -> List[ConflictType]:
    """‚öîÔ∏è Erkennt verschiedene Arten von ethischen Konflikten"""
   
    conflicts = []
   
    # Prinzipien-Konflikte (niedrige Scores in mehreren ALIGN-Bereichen)
    low_scores = [step for step in step_analyses if step.score < 0.6]
    if len(low_scores) >= 2:
        conflicts.append(ConflictType.PRINCIPLE_CONFLICT)
   
    # Stakeholder-Konflikte
    if stakeholder_analysis and stakeholder_analysis.potential_conflicts:
        conflicts.append(ConflictType.STAKEHOLDER_CONFLICT)
   
    # Werte-Konflikte (erkennbar an spezifischen Bedenken)
    value_conflict_indicators = ['moral dilemma', 'ethical conflict', 'competing values']
    for step in step_analyses
```