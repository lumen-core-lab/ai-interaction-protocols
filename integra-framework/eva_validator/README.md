# EVA Validator v1.0

**E**thical **V**alidation & **A**udit System - Universelles Validierungssystem fÃ¼r ethische KI-Entscheidungen

## ğŸ¯ Was ist EVA?

EVA Validator ist ein modulares, universell einsetzbares System zur ethischen Validierung von KI-Entscheidungen. Es funktioniert mit **jeder KI** - egal ob ChatGPT, Claude, INTEGRA oder eigene Systeme.

## âœ¨ Features

- âœ… **Universell**: Funktioniert mit jeder KI
- ğŸ” **Ethik-PrÃ¼fung**: Bewertet Entscheidungen nach ethischen Kriterien
- ğŸš¨ **Eskalation**: Automatische Weiterleitung kritischer FÃ¤lle
- ğŸ“ **Audit-Trail**: VollstÃ¤ndige Protokollierung
- ğŸ“Š **Feedback**: Lernsignale fÃ¼r KI-Verbesserung
- âš™ï¸ **Konfigurierbar**: Anpassbare Schwellenwerte und Regeln

## ğŸš€ Schnellstart

### 1. Installation

```bash
# PyYAML installieren (optional, fÃ¼r YAML-Konfigs)
pip install pyyaml
```

### 2. Einfachste Verwendung

```python
from eva_validator import run_eva

result = run_eva({
    "id": "test-001",
    "input": "Kann ich lÃ¼gen?",
    "output": "Nein, Ehrlichkeit ist wichtig.",
    "score": 0.9,
    "explanation": "FÃ¶rdert ethisches Verhalten"
})

print(f"Validiert: {result['validated']}")  # True
print(f"Empfehlung: {result['recommendation']}")
```

### 3. Demo ausfÃ¼hren

```bash
# Einfache Demo
python einfache_demo.py

# VollstÃ¤ndige interaktive Demo
python examples/vollstaendige_demo.py
```

## ğŸ“‚ Dateistruktur

```
eva_validator/
â”œâ”€â”€ __init__.py          # Package-Definition
â”œâ”€â”€ schema.py            # Ein-/Ausgabeformate
â”œâ”€â”€ evaluator.py         # Bewertungslogik
â”œâ”€â”€ escalation.py        # Eskalationsmanagement
â”œâ”€â”€ validator.py         # Hauptvalidierung
â”œâ”€â”€ logger.py            # Audit-Logging
â”œâ”€â”€ feedback.py          # Feedback-System
â”œâ”€â”€ config.py            # Konfiguration
â”œâ”€â”€ einfache_demo.py     # Einfache Demo
â””â”€â”€ examples/
    â””â”€â”€ vollstaendige_demo.py  # AusfÃ¼hrliche Demo
```

## ğŸ”§ Integration in deine KI

```python
from eva_validator import EVAValidator

class MeineKI:
    def __init__(self):
        self.validator = EVAValidator()
    
    def antworten(self, frage):
        # KI generiert Antwort
        antwort = self.generiere_antwort(frage)
        score = self.berechne_ethik_score(antwort)
        
        # EVA validiert
        result = self.validator.validate({
            "id": "123",
            "input": frage,
            "output": antwort,
            "score": score,
            "explanation": "KI-generiert"
        })
        
        if result.validated:
            return antwort
        else:
            return "Diese Antwort kann ich aus ethischen GrÃ¼nden nicht geben."
```

## âš™ï¸ Konfiguration

Standard-Schwellenwerte anpassen:

```python
from eva_validator import create_validator, get_default_config

# Config laden und anpassen
config = get_default_config()
config["evaluation_criteria"]["min_ethic_threshold"] = 0.8  # Strengere Bewertung

# Validator mit Custom-Config
validator = create_validator(config=config)
```

## ğŸ“Š Szenarien

EVA unterstÃ¼tzt verschiedene Szenarien mit angepassten Regeln:

- **privacy**: Datenschutz (strenge Regeln)
- **harm**: Schadenspotenzial (sehr streng)
- **education**: Bildung (ausgewogen)
- **deception**: TÃ¤uschung (kritisch)
- **general**: Allgemein (Standard)

## ğŸ›¡ï¸ Sicherheit

- Alle Entscheidungen werden protokolliert
- Kritische FÃ¤lle werden eskaliert
- Audit-Trail fÃ¼r Compliance
- Konfigurierbare Schwellenwerte

## ğŸ“ Lizenz

CC BY-NC-SA 4.0 - Kostenlos fÃ¼r nicht-kommerzielle Nutzung

## ğŸ¤ Beitragen

Verbesserungen sind willkommen! EVA ist modular aufgebaut und leicht erweiterbar.

## â“ Hilfe

Bei Problemen:
1. Stelle sicher, dass alle Dateien im `eva_validator` Ordner sind
2. Python 3.7+ ist erforderlich
3. FÃ¼hre `python einfache_demo.py` fÃ¼r einen Test aus

---

**EVA Validator** - FÃ¼r ethische und verantwortungsvolle KI ğŸŒŸ